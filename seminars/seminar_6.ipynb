{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanochkinl/ml2_course/blob/main/seminars/seminar_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c407c568-0139-4119-8e62-77eb26bad461",
      "metadata": {
        "id": "c407c568-0139-4119-8e62-77eb26bad461"
      },
      "source": [
        "## Knowledge distillation\n",
        "\n",
        "look into https://intellabs.github.io/distiller/knowledge_distillation.html for reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "18395a6a-32cf-41f7-9438-1f8dbf7dfd00",
      "metadata": {
        "id": "18395a6a-32cf-41f7-9438-1f8dbf7dfd00",
        "outputId": "14e8ac0b-07e6-42f9-e606-bbd59ff7c529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "69ef90f8-5832-4672-87a5-7c12528ca06f",
      "metadata": {
        "id": "69ef90f8-5832-4672-87a5-7c12528ca06f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed()\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2b6480c9-e7e6-4552-8cb0-2db29f32777d",
      "metadata": {
        "id": "2b6480c9-e7e6-4552-8cb0-2db29f32777d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
        "    losses = AverageMeter()\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        y_preds = model(images)\n",
        "        loss = criterion(y_preds, labels)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1000)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 100 == 0 or step == (len(valid_loader)-1):\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Grad: {grad_norm:.4f}  '\n",
        "                  .format(\n",
        "                   epoch+1, step, len(train_loader), loss=losses,\n",
        "                   grad_norm=grad_norm,\n",
        "                   ))\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    losses = AverageMeter()\n",
        "    scores = AverageMeter()\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    gt_labels = []\n",
        "    for step, (images, labels) in enumerate(valid_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        # compute loss\n",
        "        with torch.no_grad():\n",
        "            y_preds = model(images)\n",
        "        loss = criterion(y_preds, labels)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        # record accuracy\n",
        "        preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
        "        gt_labels.append(labels.to('cpu').numpy())\n",
        "        if step % 100 == 0 or step == (len(valid_loader)-1):\n",
        "            print('EVAL: [{0}/{1}] '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  .format(\n",
        "                   step, len(valid_loader),\n",
        "                   loss=losses,\n",
        "                   ))\n",
        "    predictions = np.concatenate(preds)\n",
        "    gt_labels = np.concatenate(gt_labels)\n",
        "    return losses.avg, predictions, gt_labels\n",
        "\n",
        "\n",
        "def inference_func(model, testloader, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    gt_labels = []\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    for step, (images, labels) in enumerate(testloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "        preds.append(outputs.softmax(1).to('cpu').numpy().argmax(1))\n",
        "        gt_labels.append(labels.to('cpu').numpy())\n",
        "    predictions = np.concatenate(preds)\n",
        "    gt_labels = np.concatenate(gt_labels)\n",
        "    return predictions, gt_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "47947a87-1128-4cf0-a8c9-4bef4cc3030d",
      "metadata": {
        "id": "47947a87-1128-4cf0-a8c9-4bef4cc3030d",
        "outputId": "ce4e28da-473b-4987-ff1c-cc2b438f4939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    transforms.RandomHorizontalFlip()\n",
        "])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "#val split\n",
        "train_data, val_data = train_test_split(trainset, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "da282042-4ef2-495f-a0e4-9c1e5a7d1392",
      "metadata": {
        "id": "da282042-4ef2-495f-a0e4-9c1e5a7d1392"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, model_name='resnet18', pretrained=False):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
        "        n_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(n_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "beb5342e-28d5-4449-afe9-a6fb5961654c",
      "metadata": {
        "id": "beb5342e-28d5-4449-afe9-a6fb5961654c",
        "outputId": "b976c14a-1531-47a0-8365-cfef5ad6db6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ca1f4a6b-3a70-471c-9421-2c5ac8d57f43",
      "metadata": {
        "id": "ca1f4a6b-3a70-471c-9421-2c5ac8d57f43",
        "outputId": "c2b609e5-39fe-4280-e50f-42296c501c3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/586] Loss: 2.2497(2.2497) Grad: 6.5535  \n",
            "Epoch: [1][100/586] Loss: 1.4357(1.9442) Grad: 2.7849  \n",
            "Epoch: [1][195/586] Loss: 1.2274(1.6479) Grad: 2.7652  \n",
            "Epoch: [1][200/586] Loss: 1.3355(1.6369) Grad: 3.2524  \n",
            "Epoch: [1][300/586] Loss: 1.2439(1.4538) Grad: 2.1474  \n",
            "Epoch: [1][400/586] Loss: 0.8965(1.3249) Grad: 1.9394  \n",
            "Epoch: [1][500/586] Loss: 0.9076(1.2311) Grad: 2.2591  \n",
            "EVAL: [0/196] Loss: 0.6541(0.6541) \n",
            "EVAL: [100/196] Loss: 0.8874(0.7292) \n",
            "EVAL: [195/196] Loss: 1.0097(0.7312) \n",
            "Validation accuracy for epoch 0 is 0.75\n",
            "Epoch: [2][0/586] Loss: 0.6220(0.6220) Grad: 1.6075  \n",
            "Epoch: [2][100/586] Loss: 0.6498(0.6661) Grad: 2.0414  \n",
            "Epoch: [2][195/586] Loss: 0.5228(0.6557) Grad: 1.5723  \n",
            "Epoch: [2][200/586] Loss: 0.9139(0.6547) Grad: 2.1913  \n",
            "Epoch: [2][300/586] Loss: 0.6079(0.6547) Grad: 1.5550  \n",
            "Epoch: [2][400/586] Loss: 0.6828(0.6455) Grad: 1.7500  \n",
            "Epoch: [2][500/586] Loss: 0.6820(0.6374) Grad: 1.6213  \n",
            "EVAL: [0/196] Loss: 0.4128(0.4128) \n",
            "EVAL: [100/196] Loss: 0.6843(0.5981) \n",
            "EVAL: [195/196] Loss: 1.0650(0.6021) \n",
            "Validation accuracy for epoch 1 is 0.79\n",
            "Epoch: [3][0/586] Loss: 0.3612(0.3612) Grad: 1.4254  \n",
            "Epoch: [3][100/586] Loss: 0.3948(0.4329) Grad: 1.8127  \n",
            "Epoch: [3][195/586] Loss: 0.5013(0.4527) Grad: 1.7537  \n",
            "Epoch: [3][200/586] Loss: 0.4722(0.4529) Grad: 1.5328  \n",
            "Epoch: [3][300/586] Loss: 0.6357(0.4530) Grad: 1.7058  \n",
            "Epoch: [3][400/586] Loss: 0.4290(0.4563) Grad: 1.8748  \n",
            "Epoch: [3][500/586] Loss: 0.4295(0.4616) Grad: 1.9155  \n",
            "EVAL: [0/196] Loss: 0.4729(0.4729) \n",
            "EVAL: [100/196] Loss: 0.5373(0.5539) \n",
            "EVAL: [195/196] Loss: 0.9029(0.5654) \n",
            "Validation accuracy for epoch 2 is 0.81\n",
            "Epoch: [4][0/586] Loss: 0.3422(0.3422) Grad: 1.6159  \n",
            "Epoch: [4][100/586] Loss: 0.3767(0.3131) Grad: 2.0840  \n",
            "Epoch: [4][195/586] Loss: 0.3797(0.3208) Grad: 1.9432  \n",
            "Epoch: [4][200/586] Loss: 0.4433(0.3217) Grad: 1.9089  \n",
            "Epoch: [4][300/586] Loss: 0.3046(0.3283) Grad: 1.6246  \n",
            "Epoch: [4][400/586] Loss: 0.3170(0.3347) Grad: 1.5180  \n",
            "Epoch: [4][500/586] Loss: 0.2651(0.3383) Grad: 1.4877  \n",
            "EVAL: [0/196] Loss: 0.4464(0.4464) \n",
            "EVAL: [100/196] Loss: 0.4660(0.5156) \n",
            "EVAL: [195/196] Loss: 0.8708(0.5267) \n",
            "Validation accuracy for epoch 3 is 0.82\n",
            "Epoch: [5][0/586] Loss: 0.2151(0.2151) Grad: 1.0463  \n",
            "Epoch: [5][100/586] Loss: 0.2392(0.2328) Grad: 1.3127  \n",
            "Epoch: [5][195/586] Loss: 0.6532(0.2407) Grad: 2.5630  \n",
            "Epoch: [5][200/586] Loss: 0.2486(0.2408) Grad: 1.4060  \n",
            "Epoch: [5][300/586] Loss: 0.1580(0.2474) Grad: 1.3154  \n",
            "Epoch: [5][400/586] Loss: 0.3887(0.2561) Grad: 2.0320  \n",
            "Epoch: [5][500/586] Loss: 0.3322(0.2666) Grad: 1.6017  \n",
            "EVAL: [0/196] Loss: 0.5152(0.5152) \n",
            "EVAL: [100/196] Loss: 0.4849(0.5382) \n",
            "EVAL: [195/196] Loss: 0.8623(0.5484) \n",
            "Validation accuracy for epoch 4 is 0.82\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "teacher_model = ResNet(model_name='resnet34', pretrained=True)\n",
        "teacher_model.to(device)\n",
        "optimizer = optim.Adam(teacher_model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "    # train\n",
        "    avg_loss = train_fn(train_loader, teacher_model, criterion, optimizer, epoch, device)\n",
        "    # eval\n",
        "    avg_val_loss, preds, val_labels = valid_fn(valid_loader, teacher_model, criterion, device)\n",
        "    acc_score = accuracy_score(val_labels, preds.argmax(1))\n",
        "    print(f\"Validation accuracy for epoch {epoch} is {acc_score:.2f}\")\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539ab516-39ec-414f-a804-58da5c777844",
      "metadata": {
        "id": "539ab516-39ec-414f-a804-58da5c777844"
      },
      "source": [
        "Knowledge distillation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Temperature Illustration { run: \"auto\" }\n",
        "from seaborn import barplot\n",
        "\n",
        "temp = 1.21 # @param {type:\"slider\", min:0.01, max:100, step:0.1}\n",
        "\n",
        "barplot(F.softmax(torch.tensor([0.1, 0.3, 0.4, 0.6, 0.89]) / temp, dim=0).numpy())"
      ],
      "metadata": {
        "id": "CQQgHEia8PG0",
        "outputId": "be96d55d-7d0d-42ea-bd05-0df4854b6a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "id": "CQQgHEia8PG0",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "ba0ec3dd-43e0-4ca4-a6f8-3203b962a83c",
      "metadata": {
        "id": "ba0ec3dd-43e0-4ca4-a6f8-3203b962a83c"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def calculate_kd_loss(logits_student, logits_teacher, y_true, temp=1, distil_weight=.5):\n",
        "    soft_teacher_out = F.softmax(logits_teacher, dim=1)\n",
        "    soft_student_out = F.softmax(logits_student, dim=1)\n",
        "    student_loss = F.cross_entropy(logits_student, y_true)\n",
        "    distillation_loss =  F.cross_entropy(\n",
        "        soft_student_out / temp, soft_teacher_out / temp\n",
        "    ) * temp**2\n",
        "    return (1 - distil_weight) * student_loss + distil_weight * distillation_loss\n",
        "\n",
        "\n",
        "def train_student_with_kd(\n",
        "        train_loader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        epochs,\n",
        "        device,\n",
        "        student_model,\n",
        "        teacher_model,\n",
        "    ):\n",
        "        teacher_model.eval()\n",
        "        student_model.train()\n",
        "        loss_arr = []\n",
        "        length_of_dataset = len(train_loader.dataset)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        print(\"Training Student...\")\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "            correct = 0\n",
        "\n",
        "            for (data, label) in train_loader:\n",
        "\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                student_out = student_model(data)\n",
        "                teacher_out = teacher_model(data)\n",
        "\n",
        "                loss = calculate_kd_loss(student_out, teacher_out, label)\n",
        "\n",
        "                if isinstance(student_out, tuple):\n",
        "                    student_out = student_out[0]\n",
        "\n",
        "                pred = student_out.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "                loss.backward()\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1000)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_val_loss, preds, val_labels = valid_fn(valid_loader, student_model, nn.CrossEntropyLoss(), device)\n",
        "            acc_score = accuracy_score(val_labels, preds.argmax(1))\n",
        "            print(f\"Validation accuracy for epoch {ep} is {acc_score:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "647c0335-c38d-43e0-bdc6-cd1f90b4b18e",
      "metadata": {
        "id": "647c0335-c38d-43e0-bdc6-cd1f90b4b18e",
        "outputId": "210e726a-e38b-499e-88c2-76f4190458b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Student...\n",
            "EVAL: [0/196] Loss: 0.5785(0.5785) \n",
            "EVAL: [100/196] Loss: 0.7044(0.7113) \n",
            "EVAL: [195/196] Loss: 1.2737(0.7134) \n",
            "Validation accuracy for epoch 0 is 0.75\n",
            "EVAL: [0/196] Loss: 0.6966(0.6966) \n",
            "EVAL: [100/196] Loss: 0.7143(0.7244) \n",
            "EVAL: [195/196] Loss: 1.0807(0.7322) \n",
            "Validation accuracy for epoch 1 is 0.76\n",
            "EVAL: [0/196] Loss: 0.5887(0.5887) \n",
            "EVAL: [100/196] Loss: 0.7085(0.6493) \n",
            "EVAL: [195/196] Loss: 0.8057(0.6611) \n",
            "Validation accuracy for epoch 2 is 0.79\n",
            "EVAL: [0/196] Loss: 0.5028(0.5028) \n",
            "EVAL: [100/196] Loss: 0.5726(0.5860) \n",
            "EVAL: [195/196] Loss: 0.9814(0.5996) \n",
            "Validation accuracy for epoch 3 is 0.80\n",
            "EVAL: [0/196] Loss: 0.5375(0.5375) \n",
            "EVAL: [100/196] Loss: 0.8499(0.6877) \n",
            "EVAL: [195/196] Loss: 1.1250(0.7013) \n",
            "Validation accuracy for epoch 4 is 0.79\n",
            "EVAL: [0/196] Loss: 0.7315(0.7315) \n",
            "EVAL: [100/196] Loss: 0.7271(0.6848) \n",
            "EVAL: [195/196] Loss: 1.2397(0.6859) \n",
            "Validation accuracy for epoch 5 is 0.79\n",
            "EVAL: [0/196] Loss: 0.5790(0.5790) \n",
            "EVAL: [100/196] Loss: 0.7438(0.6527) \n",
            "EVAL: [195/196] Loss: 1.1720(0.6540) \n",
            "Validation accuracy for epoch 6 is 0.81\n",
            "EVAL: [0/196] Loss: 0.8238(0.8238) \n",
            "EVAL: [100/196] Loss: 0.6582(0.6765) \n",
            "EVAL: [195/196] Loss: 1.2648(0.6838) \n",
            "Validation accuracy for epoch 7 is 0.82\n",
            "EVAL: [0/196] Loss: 0.7459(0.7459) \n",
            "EVAL: [100/196] Loss: 0.7234(0.6976) \n",
            "EVAL: [195/196] Loss: 1.8107(0.7272) \n",
            "Validation accuracy for epoch 8 is 0.81\n",
            "EVAL: [0/196] Loss: 0.7621(0.7621) \n",
            "EVAL: [100/196] Loss: 0.7818(0.7878) \n",
            "EVAL: [195/196] Loss: 1.4760(0.7854) \n",
            "Validation accuracy for epoch 9 is 0.81\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "student_model = ResNet(model_name='resnet18', pretrained=True)\n",
        "student_model.to(device)\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=1e-3)\n",
        "train_student_with_kd(\n",
        "    train_loader,\n",
        "    partial(calculate_kd_loss, temp=2, distil_weight=0.8),\n",
        "    optimizer,\n",
        "    10,\n",
        "    device,\n",
        "    student_model,\n",
        "    teacher_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b5f1a9-d174-4889-9377-bae57ef2485b",
      "metadata": {
        "id": "02b5f1a9-d174-4889-9377-bae57ef2485b"
      },
      "source": [
        "## Active learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5e4168-3b4c-42b2-a903-bdf9a177be26",
      "metadata": {
        "id": "bc5e4168-3b4c-42b2-a903-bdf9a177be26"
      },
      "source": [
        "![1_RMZMa6FpJ18i3u9DxP4LEw.png](attachment:8a877bcf-d42c-4f96-a8a8-23e9c457ffa0.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1xrJsbLoaB2"
      },
      "id": "a1xrJsbLoaB2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [.conda-al_exp]",
      "language": "python",
      "name": "conda-env-.conda-al_exp-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}